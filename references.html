<p><span class="citation" data-cites="roijers_survey_2013">1</span> <span class="citation" data-cites="hayes_practical_2022">2</span> <span class="citation" data-cites="chen_meta-learning_2019">3</span> <span class="citation" data-cites="chauhan_brief_2023">4</span> <span class="citation" data-cites="sarafian_recomposing_2021">5</span> <span class="citation" data-cites="lu_multi_objective_2023">6</span> <span class="citation" data-cites="xu_prediction_guided_2020">7</span> <span class="citation" data-cites="alegre_sample_efficient_2023">8</span> <span class="citation" data-cites="zintgraf_utility_2015">9</span> <span class="citation" data-cites="zitzler_quality_2008">10</span> <span class="citation" data-cites="ishibuchi_modified_2015">11</span></p>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-roijers_survey_2013">
<p>[1] D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley, “A survey of multi-objective sequential decision-making,” <em>Journal of Artificial Intelligence Research</em>, vol. 48, pp. 67–113, Oct. 2013, doi: <a href="https://doi.org/10.1613/jair.3987">10.1613/jair.3987</a>.</p>
</div>
<div id="ref-hayes_practical_2022">
<p>[2] C. F. Hayes <em>et al.</em>, “A practical guide to multi-objective reinforcement learning and planning,” <em>Autonomous Agents and Multi-Agent Systems</em>, vol. 36, no. 1, Apr. 2022, doi: <a href="https://doi.org/10.1007/s10458-022-09552-y">10.1007/s10458-022-09552-y</a>.</p>
</div>
<div id="ref-chen_meta-learning_2019">
<p>[3] X. Chen, A. Ghadirzadeh, M. Björkman, and P. Jensfelt, “Meta-learning for multi-objective reinforcement learning,” in <em>2019 ieee/rsj international conference on intelligent robots and systems (iros)</em>, Macau, China: IEEE Press, 2019, pp. 977–983. doi: <a href="https://doi.org/10.1109/IROS40897.2019.8968092">10.1109/IROS40897.2019.8968092</a>.</p>
</div>
<div id="ref-chauhan_brief_2023">
<p>[4] V. K. Chauhan, J. Zhou, P. Lu, S. Molaei, and D. A. Clifton, “A brief review of hypernetworks in deep learning.” arXiv, Aug. 10, 2023. Accessed: Aug. 16, 2023. Available: <a href="http://arxiv.org/abs/2306.06955">http://arxiv.org/abs/2306.06955</a></p>
</div>
<div id="ref-sarafian_recomposing_2021">
<p>[5] E. Sarafian, S. Keynan, and S. Kraus, “Recomposing the reinforcement learning building blocks with hypernetworks,” in <em>Proceedings of the 38th international conference on machine learning</em>, M. Meila and T. Zhang, Eds., in Proceedings of machine learning research, vol. 139. PMLR, 2021, pp. 9301–9312. Available: <a href="https://proceedings.mlr.press/v139/sarafian21a.html">https://proceedings.mlr.press/v139/sarafian21a.html</a></p>
</div>
<div id="ref-lu_multi_objective_2023">
<p>[6] H. Lu, D. Herman, and Y. Yu, “Multi-objective reinforcement learning: Convexity, stationarity and pareto optimality,” in <em>The eleventh international conference on learning representations</em>, 2023. Available: <a href="https://openreview.net/forum?id=TjEzIsyEsQ6">https://openreview.net/forum?id=TjEzIsyEsQ6</a></p>
</div>
<div id="ref-xu_prediction_guided_2020">
<p>[7] J. Xu, Y. Tian, P. Ma, D. Rus, S. Sueda, and W. Matusik, “Prediction-guided multi-objective reinforcement learning for continuous robot control,” in <em>Proceedings of the 37th international conference on machine learning</em>, H. D. III and A. Singh, Eds., in Proceedings of machine learning research, vol. 119. PMLR, 2020, pp. 10607–10616. Available: <a href="https://proceedings.mlr.press/v119/xu20h.html">https://proceedings.mlr.press/v119/xu20h.html</a></p>
</div>
<div id="ref-alegre_sample_efficient_2023">
<p>[8] A. B. Alegre Lucas N, D. M. Roijers, A. Nowé, and B. C. da Silva, “Sample-efficient multi-objective learning via generalized policy improvement prioritization,” in <em>AAMAS ’23: Proceedings of the 2023 international conference on autonomous agents and multiagent systems</em>, N. Agmon, B. An, A. Ricci, and W. Yeoh, Eds., International Foundation for Autonomous Agents; Multiagent Systems, 2023, pp. 2003–2012. doi: <a href="https://doi.org/10.5555/3545946.3598872">10.5555/3545946.3598872</a>.</p>
</div>
<div id="ref-zintgraf_utility_2015">
<p>[9] L. M. Zintgraf, T. V. Kanters, D. M. Roijers, F. Oliehoek, and P. Beau, “Quality assessment of morl algorithms: A utility-based approach,” in <em>Benelearn 2015: Proceedings of the 24th annual machine learning conference of belgium and the netherlands</em>, 2015.</p>
</div>
<div id="ref-zitzler_quality_2008">
<p>[10] E. Zitzler, J. Knowles, and L. Thiele, “Quality assessment of pareto set approximations,” in <em>Multiobjective optimization: Interactive and evolutionary approaches</em>, J. Branke, K. Deb, K. Miettinen, and R. Słowiński, Eds., Berlin, Heidelberg: Springer, 2008, pp. 373–404. doi: <a href="https://doi.org/10.1007/978-3-540-88908-3_14">10.1007/978-3-540-88908-3_14</a>.</p>
</div>
<div id="ref-ishibuchi_modified_2015">
<p>[11] H. Ishibuchi, H. Masuda, Y. Tanigaki, and Y. Nojima, “Modified distance calculation in generational distance and inverted generational distance,” in <em>Evolutionary multi-criterion optimization</em>, A. Gaspar-Cunha, C. Henggeler Antunes, and C. C. Coello, Eds., in Lecture notes in computer science. Cham: Springer International Publishing, 2015, pp. 110–125. doi: <a href="https://doi.org/10.1007/978-3-319-15892-1_8">10.1007/978-3-319-15892-1_8</a>.</p>
</div>
</div>
