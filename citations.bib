@article{hayes_practical_2022,
	title        = {A practical guide to multi-objective reinforcement learning and planning},
	author       = {Hayes,  Conor F. and Rădulescu,  Roxana and Bargiacchi,  Eugenio and K\"{a}llstr\"{o}m,  Johan and Macfarlane,  Matthew and Reymond,  Mathieu and Verstraeten,  Timothy and Zintgraf,  Luisa M. and Dazeley,  Richard and Heintz,  Fredrik and Howley,  Enda and Irissappane,  Athirai A. and Mannion,  Patrick and Nowé,  Ann and Ramos,  Gabriel and Restelli,  Marcello and Vamplew,  Peter and Roijers,  Diederik M.},
	year         = 2022,
	month        = apr,
	journal      = {Autonomous Agents and Multi-Agent Systems},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 36,
	number       = 1,
	doi          = {10.1007/s10458-022-09552-y},
	issn         = {1573-7454},
	url          = {http://dx.doi.org/10.1007/s10458-022-09552-y}
}

@inproceedings{zintgraf_utility_2015,
    title       = {Quality assessment of MORL algorithms: A utility-based approach},
    author      = {Zintgraf, Luisa M and Kanters, Timon V and Roijers, Diederik M and Oliehoek, Frans and Beau, Philipp},
    booktitle   = {Benelearn 2015: proceedings of the 24th annual machine learning conference of Belgium and the Netherlands},
    year        = {2015}
}

@incollection{zitzler_quality_2008,
	title        = {Quality Assessment of Pareto Set Approximations},
	author       = {Zitzler, Eckart and Knowles, Joshua and Thiele, Lothar},
	booktitle    = {Multiobjective Optimization: Interactive and Evolutionary Approaches},
	location     = {Berlin, Heidelberg},
	publisher    = {Springer},
	pages        = {373--404},
	doi          = {10.1007/978-3-540-88908-3_14},
	isbn         = {978-3-540-88908-3},
	url          = {https://doi.org/10.1007/978-3-540-88908-3_14},
	urldate      = {2024-04-30},
	editor       = {Branke, Jürgen and Deb, Kalyanmoy and Miettinen, Kaisa and Słowiński, Roman},
	date         = 2008,
	langid       = {english},
	keywords     = {{MORL}, Pareto Front, Dominance Ranking, Multiobjective Optimizer, Objective Space, Quality Indicator}
}

@inproceedings{xu_prediction_guided_2020,
    title        = {Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control},
    author       = {Xu, Jie and Tian, Yunsheng and Ma, Pingchuan and Rus, Daniela and Sueda, Shinjiro and Matusik, Wojciech},
    booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
    pages        = {10607--10616},
    year         = {2020},
    editor       = {III, Hal Daumé and Singh, Aarti},
    volume       = {119},
    series       = {Proceedings of Machine Learning Research},
    month        = {13--18 Jul},
    publisher    = {PMLR},
    pdf          = {http://proceedings.mlr.press/v119/xu20h/xu20h.pdf},
    url          = {https://proceedings.mlr.press/v119/xu20h.html},
    abstract     = {Many real-world control problems involve conflicting objectives where we desire a dense and high-quality set of control policies that are optimal for different objective preferences (called Pareto-optimal). While extensive research in multi-objective reinforcement learning (MORL) has been conducted to tackle such problems, multi-objective optimization for complex continuous robot control is still under-explored. In this work, we propose an efficient evolutionary learning algorithm to find the Pareto set approximation for continuous robot control problems, by extending a state-of-the-art RL algorithm and presenting a novel prediction model to guide the learning process. In addition to efficiently discovering the individual policies on the Pareto front, we construct a continuous set of Pareto-optimal solutions by Pareto analysis and interpolation. Furthermore, we design seven multi-objective RL environments with continuous action space, which is the first benchmark platform to evaluate MORL algorithms on various robot control problems. We test the previous methods on the proposed benchmark problems, and the experiments show that our approach is able to find a much denser and higher-quality set of Pareto policies than the existing algorithms.}
}

@inproceedings{alegre_sample_efficient_2023,
	title        = {Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization},
	author       = {Alegre, Lucas N, And Bazzan, Ana L. C. and Roijers, Diederik M. and Nowé, Ann and da Silva, Bruno C.},
	year         = 2023,
	booktitle    = {AAMAS '23: Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
	publisher    = {International Foundation for Autonomous Agents and Multiagent Systems},
	pages        = {2003--2012},
	doi          = {10.5555/3545946.3598872},
	editor       = {Noa Agmon and Bo An and Alessandro Ricci and William Yeoh}
}

@inproceedings{ishibuchi_modified_2015,
	title        = {Modified Distance Calculation in Generational Distance and Inverted Generational Distance},
	author       = {Ishibuchi, Hisao and Masuda, Hiroyuki and Tanigaki, Yuki and Nojima, Yusuke},
	booktitle    = {Evolutionary Multi-Criterion Optimization},
	location     = {Cham},
	publisher    = {Springer International Publishing},
	series       = {Lecture Notes in Computer Science},
	pages        = {110--125},
	doi          = {10.1007/978-3-319-15892-1_8},
	isbn         = {978-3-319-15892-1},
	editor       = {Gaspar-Cunha, António and Henggeler Antunes, Carlos and Coello, Carlos Coello},
	date         = 2015,
	langid       = {english},
	keywords     = {{MOP}, Evolutionary Multiobjective Optimization, Generational distance, Inverted generational distance, Pareto compliance, Performance indicators},
	file         = {Full Text PDF:/home/sh/Zotero/storage/XLC5DZII/Ishibuchi et al. - 2015 - Modified Distance Calculation in Generational Dist.pdf:application/pdf}
}
@inproceedings{lu_multi_objective_2023,
	title        = {Multi-Objective Reinforcement Learning: Convexity, Stationarity and Pareto Optimality},
	author       = {Haoye Lu and Daniel Herman and Yaoliang Yu},
	year         = 2023,
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=TjEzIsyEsQ6},
	langid       = {english}
}
@inproceedings{sarafian_recomposing_2021,
	title        = {Recomposing the Reinforcement Learning Building Blocks with Hypernetworks},
	author       = {Sarafian, Elad and Keynan, Shai and Kraus, Sarit},
	year         = 2021,
	month        = {18--24 Jul},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 139,
	pages        = {9301--9312},
	url          = {https://proceedings.mlr.press/v139/sarafian21a.html},
	editor       = {Meila, Marina and Zhang, Tong},
	pdf          = {http://proceedings.mlr.press/v139/sarafian21a/sarafian21a.pdf}
}
@article{roijers_survey_2013,
	title        = {A Survey of Multi-Objective Sequential Decision-Making},
	author       = {Roijers, D. M. and Vamplew, P. and Whiteson, S. and Dazeley, R.},
	volume       = 48,
	pages        = {67--113},
	doi          = {10.1613/jair.3987},
	issn         = {1076-9757},
	url          = {https://www.jair.org/index.php/jair/article/view/10836},
	urldate      = {2023-09-07},
	rights       = {Copyright (c)},
	abstract     = {Sequential decision-making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision-theoretic planning and learning, which has largely focused on single-objective settings. This article surveys algorithms designed for sequential decision-making problems with multiple objectives. Though there is a growing body of literature on this subject, little of it makes explicit under what circumstances special methods are needed to solve multi-objective problems. Therefore, we identify three distinct scenarios in which converting such a problem to a single-objective one is impossible, infeasible, or undesirable. Furthermore, we propose a taxonomy that classifies multi-objective methods according to the applicable scenario, the nature of the scalarization function (which projects multi-objective values to scalar ones), and the type of policies considered. We show how these factors determine the nature of an optimal solution, which can be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we survey the literature on multi-objective methods for planning and learning. Finally, we discuss key applications of such methods and outline opportunities for future work.},
	journaltitle = {Journal of Artificial Intelligence Research},
	date         = {2013-10-18},
	langid       = {english}
}

@inproceedings{chen_meta-learning_2019,
    author       = {Chen, Xi and Ghadirzadeh, Ali and Bj\"{o}rkman, M\r{a}rten and Jensfelt, Patric},
    title        = {Meta-Learning for Multi-objective Reinforcement Learning},
    year         = {2019},
    publisher    = {IEEE Press},
    url          = {https://doi.org/10.1109/IROS40897.2019.8968092},
    doi          = {10.1109/IROS40897.2019.8968092},
    booktitle    = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    pages        = {977–983},
    numpages     = {7},
    location     = {Macau, China}
}


@misc{chauhan_brief_2023,
	title        = {A Brief Review of Hypernetworks in Deep Learning},
	author       = {Vinod Kumar Chauhan and Jiandong Zhou and Ping Lu and Soheila Molaei and David A. Clifton},
	publisher    = {{arXiv}},
	number       = {{arXiv}:2306.06955},
	urldate      = {2023-08-16},
	eprint       = {2306.06955},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
	date         = {2023-08-10},
	langid       = {english},
	eprinttype   = {arxiv},
	keywords     = {Computer Science - Machine Learning, Hyper Networks, review}
}

