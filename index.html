<!doctype html>
<html lang="en">
	<head>
		<meta charset="ut"100%">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Generalizing Pareto optimal policies in Multi-objective Reinforcement learning</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css">
        <link rel="stylesheet" href="styles/common.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
               <section id="title-page">
                 <h1>
                    Generalizing Pareto optimal policies in 
                    Multi-objective Reinforcement learning
                 </h1>
                 <h3 style="font-size: 70%"> Empirical study of hypernetworks</h3>
                 <span style="font-size: 60%"> Santeri Heiskanen</span><br>
                 <span style="text-align: center; font-size: 50%">
                    <b>Supervisors</b>: Prof. Ville Kyrki<sup>1</sup>,
                    Dr. Atanu Mazumdar <sup>1</sup>,
                    Prof. Joni Kämäräinen<sup>2</sup>
                 </span>
                 <div style="text-align: center; font-size: 40%">
                    <sup>1</sup>Aalto University, <sup>2</sup>Tampere University
                 </div>

                 <span style="text-align: center; font-size: 50%">20-07-2024</span>
               </section>
               
               <section id="intro-to-morl">
                  <h2>Multi-objective reinforcement learning in a nutshell</h2>

                  <div class="column-layout"> 
                     <div>
                        <ul>
                           <li>
                              Multi-objective reinforcement learning (MORL) is
                              an extension to single-objective reinforcement
                              learning, where one considers <span class="alert">multiple</span> 
                              (conflicting) objectives simultaneously.
                           </li>
                           <li>
                              A policy \(\pi\) is considered
                              <span class="alert">Pareto-optimal</span> if it 
                              performs better than the other policies in at least
                              one objective, and is (at least) equally good in 
                              other objectives.
                           </li>

                           <li>
                              Goal of MORL is (often) to find a set of Pareto-optimal
                              policies, called Pareto front.<sup>1</sup>
                           </li>

                           <li> <b>Why?</b>
                              <ul>
                                <li> Many real-world problems are <em>truly</em>
                                    multi-objective.
                                </li> 
                                <li>
                                   The responsibility of making the compromise
                                   is moved away from the engineer to the user.
                                </li>
                                <li>
                                   The desired solution can be selected after
                                   observing the outcomes of the different trade-offs.
                                </li>
                              </ul>
                           </li> 
                        </ul>
                     </div>
                     <div>
                        <figure>
                           <img src="./images/theory/conceptual-pareto-front.svg">
                           <figcaption>
                              <b>Figure</b>: Conceptual Pareto-front for two
                              objectives. Each point in the plot represents a
                              value of a policy \(\pi\) with given preferences
                              \(\mathbf{w}\).
                           </figcaption>
                        </figure>
                     </div>
                  </div>
                  
                  <div class="small-note">
                    1. The exact goal is dependent on multiple factors, including 
                    the type of the utility function. See Roijers et al.
                    <span class="ref" id="@roijers_survey_2013"></span> for more information.
                  </div>
                  
                  <!-- Speaker notes -->
                  <aside class="notes">
                     <ul>
                        <li>
                           Pareto-optimal policies &rArr; Performing any better
                           on a given objective would reduce performance on 
                           some other objective. (utilize the figure)
                        </li>
                        <li>
                           Goal: Find the set of Pareto-optimal policies, i.e. 
                           consider all possible trade-offs.
                        </li>
                        <li>Examples of multi-objective cases:
                           Robotics: Speed/manoeuvrability and energy consumption. 
                           Finance: Risk & Returns
                        </li>
                     </ul>
                  </aside>

               </section>
               
               <!-- HIDDEN SLIDE -->
               <section id="problem-setting-and-assumptions">
                  <h2>Problem setting</h2>
                  
                  <div class="column-layout">
                     <div>
                        <figure>
                           <figcaption class="col-title">Formal problem definition</figcaption>
                           <ul>
                              <li>The problem is formally defined as 
                                 <span class="alert">Multi-objective Markov decision process (MOMDP)</span>.
                              </li> 
                              <li>
                                 The reward function of a MOMPD is <span class="alert">vector-valued</span>
                                 and thus the value function of a policy \(\pi\)
                                 is also vector-valued.
                                 \[\begin{equation*}
                                    \mathbf{V}^{\pi} = \mathbb{E}\left[
                                       \sum_{k=0}^{\infty} \gamma^k \mathbf{r}_{k+1}\;|\;\pi, \mu
                                    \right]
                                 \end{equation*}\]
                              </li>
                              <li>
                                 <span class="alert">
                                    Utility function
                                 </span> \(u:\; \mathbb{R}^d \rightarrow \mathbb{R}\),
                                 parametrized by the user preferences \(\mathbf{w}\),
                                 is used to map the value-function into a scalar:
                                 \(\;V_u^{\pi} = u(\mathbf{V}^\pi)\)
                              </li>
                              <li>
                                 <span class="alert">Goal</span> of the MORL agent is to optimize
                                 the scalarized (discounted) cumulative return.
                              </li>
                           </ul>
                        </figure>
                     </div>
                     
                     <div>
                        <figure>
                           <figcaption class="col-title">Assumptions</figcaption>
                           <ul>
                              <li>
                                 The user preferences \(\mathbf{w}\) are not known in advance.
                              </li>
                              <li>
                                 The utility function is as linear combination
                                 of the preferences over the objectives:
                                 \(u(\mathbf{r}, \mathbf{w}) = \mathbf{w}^T \mathbf{r}\)
                              </li>
                              <li>
                                 The preferences are presented as a non-zero vector
                                 \(\mathbf{w}\), where
                                 \[\begin{equation*}
                                 0 \leq w_i \leq 1,\; i=1,\dots, n\text{ and } \sum_{i=1}^n w_i = 1
                                 \end{equation*}\]
                              </li>
                           </ul>
                        </figure>
                     </div>
                  </div>

                  <aside class="notes">
                    <ul>
                       <li>
                          Optimizing reward-valued value-function is not
                          possible &rArr; Requires utility function
                       </li>
                       <li>
                          Goal: Optimize the cumulative scalarized reward with
                          the given preferences
                       </li>
                       <li>
                          w not known in advance &rArr; One needs to find all 
                          possible trade-offs
                       </li>
                       <li>
                          Linear utility function simplifies problem
                          significantly.
                       </li>
                    </ul> 
                  </aside>
               </section>

               <section id="research-question">
                  <h2>Research questions</h2>
                  <div class="column-layout">
                     <figure>
                       <figcaption class="col-title">Motivation</figcaption> 
                       <ul>
                           <li> Current approaches for finding Pareto-front
                              either produce a set of policies naturally 
                              (<em>inner-loop</em> methods), or repeatedly
                              solve a single objective problem with different
                              scalarizations of the objective (<em>outer-loop</em> methods).
                              <span class="ref" id="@hayes_practical_2022"></span>
                           </li> 
                           <li>
                              The inherit simplicity of outer-loop methods is 
                              appealing. However, they suffer from
                              <span class="alert">inefficiency</span> since a
                              new policy is trained for each scalarization of
                              the goal.
                           </li>

                           <li>
                              The optimal policies for different user preferences
                              can differ significantly. Previous attemps to 
                              generalize learned information to new policies 
                              with different preferences over the objectives
                              (e.g. Meta-policy approach by Chen et al. 
                              <span class="ref" id="@chen_meta-learning_2019"></span>
                              )
                              have suffered from sub-optimal performance.
                           </li>
                       </ul>
                     </figure>
                     <figure>
                        <figcaption class="col-title">Research questions</figcaption>
                        <ul>
                          <li>
                             Can the learned information from the previous 
                             Pareto optimal policies be generalized to new 
                             policies with different preferences over the objectives,
                             while retaining the <em>quality</em> of the solution set?
                          </li> 
                          <li>
                             Can the information sharing improve the efficiency
                             of the training process?
                          </li>
                        </ul>
                     </figure>
                  </div>
                  <!-- Speaker notes -->
                  <aside class="notes">
                     <ul>
                        <li>
                           Two approaches for solving: Inner and outer-loop methods
                        </li>
                        <li>
                           Outer-loop methods inefficient, since no information
                           sharing. 
                        </li>
                        <li>
                           Optimal policies can differ significantly.
                        </li>
                     </ul>
                  </aside>
               </section>

               <section id="solution-proposal">
                  <h2>Solution proposal</h2>
                  <div class="master-grid">
                     <div class="master-grid-text">
                        <ul>
                           <li>
                              <span class="alert">Hypernetworks</span> have shown 
                              excellent performance in many different tasks,
                              including traditional RL <span class="ref" id="@chauhan_brief_2024">.
                           </li>
                           <li>
                              <span class="alert">Hypothesis</span>: natural
                              information sharing of hypernetworks could improve 
                              the generalization capabilities of Pareto-optimal
                              policies.
                           </li>
                           <li>
                              Two different configurations for the critic:
                              <ul>
                                 <li>
                                    <b>A-critic-MO-hyper</b>: The critic
                                    network takes in only action \(a\) as its input.
                                 </li>
                                 <li>
                                    <b>SAP-critic-MO-hyper</b>: The critic network
                                    takes in <b>s</b>tate \(s\), <b>a</b>ction \(a\)
                                    and <b>p</b>references \(\mathbf{w}\) as its inputs.
                                 </li>
                                 <li>
                                    In both cases the hypernetwork takes 
                                    the <b>s</b>tate \(s\) and <b>p</b>references
                                    \(\mathbf{w}\) as it's input.
                                 </li>
                              </ul>
                           </li>
                           <li>
                              The policies are trained using an existing MORL algorithm,
                              called Concave Augmented Pareto Q-learning (CAPQL)
                              <span class="ref" id="@lu_multi_objective_2023"></span>.
                           </li>
                        </ul> 
                     </div>
                     
                        <figure class="master-grid-fig-1">
                           <img src="images/theory/model-structure.svg" alt="" height=229>
                           <figcaption>
                              <b>Figure a)</b> Following the work of Sarafian et al.
                                 <span class="ref" id="@sarafian_recomposing_2021"></span>,
                                 the critic is modeled as a <em>Contextual bandit</em>
                                 \(Q_{s, \mathbf{w}}(a)\).
                           </figcaption>
                        </figure>
                        
                        <figure class="master-grid-fig-2">
                           <img src="images/theory/architecture.svg" alt="">
                           <figcaption>
                              <b>Figure b)</b> The hypernetwork consists of multiple
                              residual network plots. Each head of the hypernetwork
                              produces the parameters for a single layer of the 
                              critic network.
                           </figcaption>
                        </figure>
                     </div>
                     
                     <!-- Speaker notes -->
                     <aside class="notes">
                        <ul>
                           <li>Hypernetworks are neural networks that produce weights for another network.</li>
                           <li>This scheme allows for soft-information sharing AND for continuous Pareto-front.</li>
                           <li>
                              Two configurations: This is done to see if the 
                              encoding of the information in the weights is enough.
                           </li>
                           <li>
                              Hypernetwork is a ResNet with 9M parameters,
                              again motivated by Sarafian et al.
                           </li>
                        </ul>
                     </aside>
                  
               </section>

               <section id="experiment-setup">
                  <h2>Experiment setup</h2>
                        <div class="two-col-bottom-row-layout">
                        <figure class="tc-br-col1">
                           <figcaption class="col-title">Baseline methods</figcaption>
                           <ul>
                              <li class="title-color">
                                 Concave augmented Pareto Q-learning (<b>CAPQL</b>)
                                 <span class="ref", id="@lu_multi_objective_2023"></span>
                              </li>
                              <ul>
                                 <li>
                                    Uses 2-layer MLP for Gaussian policy, and
                                    the critic network. 
                                 </li>
                              </ul>
                              <li class="title-color">CAPQL Large</li>
                              <ul>
                                 <li>
                                    CAPQL with 3-layer MLP with &asymp; 9M parameters
                                    for the critic network (Similar to the A-critic-MO-hyper).
                                 </li>
                              </ul>

                              <li class="title-color">
                                 Prediction guided multi-objective 
                                 reinforcement learning (<b>PGMORL</b>)
                                 <span class="ref" id="@xu_prediction_guided_2020"></span>
                              </li> 
                             <ul>
                                 <li>
                                    Evolutionary approach that predicts on 
                                    which preferences to train on. 
                                 </li>
                             </ul>
                             <li class="title-color">
                                General Policy improvement with linear support 
                                (<b>GPI-LS</b>)<span class="ref" id="@alegre_sample_efficient_2023"></span>
                             </li>
                             <ul>
                                <li>
                                   Has theoretical guarantees on which preferences
                                   should be trained on. Known to be sample 
                                   efficient.
                                </li>
                             </ul>
                           </ul>
                        </figure>

                        <figure class="tc-br-col2">
                           <figcaption class="col-title">Benchmark tasks</figcaption>
                           <ul>
                              <li class="title-color">Multi-objective Half cheetah</li>
                              <ul>
                                 <li>Two objectives: Energy efficiency &amp; forward speed.</li> 
                                 <li>
                                    Pareto-front consists of 4 different policy families.
                                    <span class="ref" id="@xu_prediction_guided_2020"></span>
                                 </li>
                              </ul>
                              <li class="title-color">Multi-objective Hopper</li>
                              <ul>
                                 <li>Two objectives: Jump height &amp; forward speed.</li>
                                 <li>
                                    Pareto-front consists of 2 different policy families.
                                    <span class="ref" id="@xu_prediction_guided_2020"></span>
                                 </li>
                              </ul>
                              <li class="title-color">Multi-objective Swimmer</li>
                              <ul>
                                <li>Two objectives: Energy efficiency &amp; forward speed.</li> 
                                <li>
                                   Pareto-front consists of 3 different policies families.
                                   <span class="ref" id="@xu_prediction_guided_2020"></span>
                                </li>
                              </ul>
                              <li>
                                 Increased amount of policy families indicates
                                 larger variance between the optimal behaviours
                                 for different preferences.
                              </li>
                              <li>
                                 Exact environment definitions adapted from Xu
                                 et al. <span class="ref" id="@xu_prediction_guided_2020"></span>
                              </li>
                           </ul>
                        </figure>

                        <!--- Bottom row --->
                        <figure class="tc-br-bottom-row-1">
                           <img src="images/env_plots/halfcheetah_v4.svg" alt="Image of the Half cheetah environment" height=250>
                           <figcaption><b>Figure a)</b>: The Half cheetah environment</figcaption>
                        </figure> 

                        <figure class="tc-br-bottom-row-2">
                           <img src="images/env_plots/hopper_v4.svg" alt="Image of the Hopper environment" height=250>
                           <figcaption><b>Figure b)</b>: The Hopper environment</figcaption>
                        </figure> 

                        <figure class="tc-br-bottom-row-3">
                           <img src="images/env_plots/swimmer_v4.svg" alt="Image of the Swimmer environment" height=250>
                           <figcaption><b>Figure c)</b>: The Swimmer environment</figcaption>
                        </figure> 
                     </div>

                     <aside class="notes">
                       <ul>
                          <li>
                           CAPQL and CAPQL large base methods. Large tests if 
                           any improvements are due to parameter count or model architecture.
                          </li>
                           <li>
                              PGMORL previous SOTA on continuous control. Trains 
                              discrete, separate policies. Sample Inefficient.
                           </li> 
                           <li>
                              GPI-LS is a quite new method, provides SOTA
                              sample-efficiency. Has theoretical guarantees
                              on which preferences need to be considered.
                           </li>
                           <li>
                              The PGMORL paper describes that the Pareto-front 
                              can be partitioned into distinct families. Can be 
                              seen as an indicator on how different the policies
                              along the front are.
                           </li>
                       </ul> 
                     </aside>
               </section>
               
               <section id="performance-indicators">
                  <h2>Performance indicators</h2>
                  <div class="column-layout">
                     <ul>
                       <li>
                           <b>Hypervolume</b>&uArr; measures the volume of 
                           the Pareto-dominated undominated solutions over the
                           possible in an approximate coverage set. This metric
                           combines the <em>uniformity</em>, <em>spread</em>
                           and <em>convergence</em> of the solution set and is able
                           to indicate improvement in any of them.
                           <span class="ref" id="@zintgraf_utility_2015"></span>.
                           Formally, the hypervolume is defined as:
                           \[\begin{equation*}
                              I^*_H(\mathcal{S}) =  
                                 \int_{\mathbf{z}_{\mathrm{lower}}}^{\mathbf{z}_{\mathrm{upper}}}
                                 \mathbf{1}_{\mathcal{S}}(\mathbf{z})d\mathbf{z}
                           \end{equation*}\]
                           where \(\mathcal{S}\) is the solution set and 
                           \(\mathbf{1}\) is an indicator function that is 1, when 
                           the vector \(\mathbf{z}\) is dominated by some vector
                           \(\mathbf{s}\) in the solution set.
                           <span class="ref" id="@zitzler_quality_2008"></span>
                       </li>

                       <li>
                          <b>Sparsity</b>&dArr; measures how sparse
                          the produced solution set is. It is defined as the 
                          average distance between two adjacent points in the 
                          solution space:
                          \[\begin{equation*}
                              Sp(\mathcal{S}) = \frac{1}{|\mathcal{S}| - 1}
                                 \sum_{j=1}^m \sum_{i=1}^{|\mathcal{S}|}(
                                    \tilde{S}_j(i) - \tilde{S}_i(i+1)
                                 )^2
                          \end{equation*}\]
                          where \(\mathcal{S}\) is the approximated solution set.
                          Since one always desires denser approximation of the 
                          Pareto-front, a lower sparsity is better.
                          <span class="ref" id="@xu_prediction_guided_2020"></span>
                       </li>
                     </ul>

                     <ul>
                       <li>
                          <b>Expected utility metric (EUM)</b>&uArr; describes the 
                          amount of utility the agent is expected to provide
                          to the user on average, given the used family
                          of utility functions. More formally, EUM is defined
                          as 
                          \[\begin{equation*}
                               \mathrm{EUM} = \mathbb{E}_{P_u}\left[ 
                                    \underset{\mathbf{V}^\pi \in \mathcal{S}}{\max}\; u(\mathbf{V}^\pi)
                                 \right]
                           \end{equation*}\]
                           where \(\mathcal{S}\) is the solution set, \(u\) is the 
                           utility function and \(P_u\) is the distribution over
                           the utility functions.
                           <span class="ref" id="@zintgraf_utility_2015"></span>
                       </li>

                       <li>
                          <b>Inverted generational distance + (IGD+)</b>&dArr;
                          measures the performance of the given approximation
                          of the Pareto-front by measuring the average
                          distance of a point \(z_i\) from a reference 
                          point set \(Z = \{z_1, z_2, \dots, z_m\}\) to the nearest
                          point \(a_i\) in the solution set:
                          \[\begin{equation*}
                              IGD^+(\mathcal{S}) = \frac{1}{|Z|}\sum_{j=1}^{|Z|} d^+(\mathbf{z}, \mathbf{s}_{j(k)})
                          \end{equation*}\]
                          where \(d^+_j = \sqrt{\sum_{i=1}^m \max\{z_j - s_j, 0\}}\) 
                          and \(\mathbf{s}_{j(k)}\) is the nearest solution point
                          to \(\mathbf{z}_k\).
                          <span class="ref" id="@ishibuchi_modified_2015"></span>
                       </li>
                     </ul>
                  </div>
                  
                  <aside class="notes">
                    <ul>
                       <li>Optimal Pareto-front has a good coverage, is dense and converged.</li>
                       <li>Hypervolume: Combines spread, uniformity and convergence</li> 
                       <li>Sparsity measures the density of the set. Used since hypervolume cannot notice gaps.</li>
                       <li>EUM: Utility based viewpoint</li>
                       <li>IGD+: Comparison to an "expert" solution.</li>
                    </ul> 
                  </aside>

               </section>

               <section id="performance">
                  <h2>Performance</h2>
                  <div class="two-col-bottom-fig-layout">
                     <div class="two-col-bottom-fig-layout-text text-vbox">
                        <ul>
                           <li>
                              Overall, the hypernetworks <span class="alert">do not</span> improve
                              the final performance of the algorithms.</li>
                           <li>
                              Moreover, the increase in the parameter count
                              results in larger run to run variance.
                           </li>
                           <li>
                              Passing the state and preferences explicitly to
                              the critic improves the performance especially
                              in the more complicated environments.
                           </li>
                           <li>
                              Interestingly, the hypernetwork based approaches
                              tend to overfit on the energy efficiency objective
                              on Halfcheetah and Swimmer as one can seen in 
                              figure b).
                           </li>
                        </ul>
                     </div>
                     <figure class="two-col-bottom-fig-layout-fig1">
                           <img src="./images/aggregate_figs/combined_results_talk.svg" alt="">
                           <figcaption>
                              <b>Figure a)</b> The interquartile mean (IQM) scores
                              with 95% bootstraped confidence intervals calculated
                              over 5 runs for each of the used environments. 
                              The GPI-LS was trained for 300k timesteps,
                              PGMORL for 9M timesteps and rest for 1.2M timesteps.
                              </figcaption>
                        </figure>

                        <figure class="two-col-bottom-fig-layout-fig2">
                           <img src="./images/aggregate_figs/pareto_front_small_talk.svg" alt="" height=200>
                           <figcaption>
                              <b>Figure b)</b>:
                              Examples of the solution sets produced by the algorithms.
                              Note the clustering on the energy efficiency objective
                              in Half cheetah and Swimmer.
                           </figcaption>
                        </figure>
                  </div>
               </section>
               
               <section id="sample-efficiency">
                  <h2>Sample efficiency</h2> 
                  <div class="four-six-hsplit">
                     <div class="text-vbox">
                        <ul>
                           <li>
                              The hypernetwork based approaches improve the 
                              convergence speed in Hopper and Halfcheetah, while
                              none of the methods were able to learn any reasonable
                              policies in Swimmer.
                           </li>
                           <li>
                              The increased run-to-run variance introduced
                              by the larger models is noticeable in all environments.
                           </li>
                           <li>
                              In Hopper, the CAPQL Large and hypernetwork
                              based approaches perform similarly, indicating that
                              the performance improvements might be due to 
                              increased parameter count instead of hypernetwork 
                              architecture.
                           </li>
                        </ul> 
                     </div>
                        <figure>
                           <img src="./images/aggregate_figs/sample_efficiency_talk_new.svg" alt="" height=450>
                           <figcaption>
                              <b>Figure</b>:
                              The IQM values of the performance metrics with
                              95% bootstrapped confidence intervals during 
                              training. The values were calculated over 5 runs.
                           </figcaption>
                        </figure>
                  </div>
               </section>

               <section id="rollouts">
                  <h2>Rollouts with SAP-critic-MO-hyper</h2>
                  <div class="grid-5-col">
                     
                     <!-- Hopper rollouts -->
                     <figure>
                        <figcaption>\(\mathbf{w} = \{1.0, 0.0\}\)</figcaption>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/hopper_sap-critic-sp-hyper-pref_0.mp4" type="video/mp4">
                        </video>
                     </figure>
                     
                     <figure>
                        <figcaption>\(\mathbf{w} = \{0.75, 0.25\}\)</figcaption>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/hopper_sap-critic-sp-hyper-pref_1.mp4" type="video/mp4">
                        </video>
                     </figure>
               

                     <figure>
                        <figcaption>\(\mathbf{w} = \{0.5, 0.5\}\)</figcaption>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/hopper_sap-critic-sp-hyper-pref_2.mp4" type="video/mp4">
                        </video>
                     </figure>
                  
                     <figure>
                        <figcaption>\(\mathbf{w} = \{0.25, 0.75\}\)</figcaption>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/hopper_sap-critic-sp-hyper-pref_3.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <figure>
                        <figcaption>\(\mathbf{w} = \{0.0, 1.0\}\)</figcaption>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/hopper_sap-critic-sp-hyper-pref_4.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <!-- Half cheetah rollouts -->
                     <figure>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/halfcheetah_sap-critic-sp-hyper-pref_0.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <figure>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/halfcheetah_sap-critic-sp-hyper-pref_1.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <figure>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/halfcheetah_sap-critic-sp-hyper-pref_2.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <figure>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/halfcheetah_sap-critic-sp-hyper-pref_3.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <figure>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/halfcheetah_sap-critic-sp-hyper-pref_4.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <!-- Swimmer rollouts -->

                     <figure>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/swimmer_sap-critic-sp-hyper-pref_0.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <figure>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/swimmer_sap-critic-sp-hyper-pref_1.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <figure>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/swimmer_sap-critic-sp-hyper-pref_2.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <figure>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/swimmer_sap-critic-sp-hyper-pref_3.mp4" type="video/mp4">
                        </video>
                     </figure>

                     <figure>
                        <video width="140" height="140" autoplay loop controls>
                           <source src="videos/swimmer_sap-critic-sp-hyper-pref_4.mp4" type="video/mp4">
                        </video>
                     </figure>
                  </div>

                  <!-- Speaker notes -->
                  <aside class="notes">
                     <ul>
                        <li>
                           In hopper, all the policies are quite similar. 
                           This was expected due to the results from PGMORL
                        </li>
                        <li>
                           In half cheetah, the learned policies nicely cover
                           the whole range of preferences.
                        </li>
                        <li>
                           In Swimmer, the policies overfit on the energy 
                           efficiency, regardless of the user preferences.
                        </li>
                     </ul>
                  </aside>
               </section>

               <section class="closer-look-at-the-training-process">
                  <h2>Closer look at the training process</h2> 
                  <div class="main-fig-text-fig-layout" style="align-items: center; justify-items: center">
                     <figure class="main-fig-text-fig-layout-fig1">
                        <img src="./images/pf_progress_sap_critic_vs_capql_talk.svg" alt="">
                        <figcaption>
                           <b>Figure a)</b>: The progress of the Pareto front 
                           during three different experiments: 
                           <b>left</b>: An failed experiment with SAP-critic-MO-hyper,
                           <b>middle</b>: An successful experiment with SAP-critic-MO-hyper,
                           <b>right</b> An successful experiment with CAPQL.
                        <figcaption>
                     </figure>

                     <figure class="main-fig-text-fig-layout-fig2">
                        <img src="./images/pref_distr_vs_reward_distr_talk.svg" alt="" height=150>
                        <figcaption>
                           <b>Figure b)</b>: The preference distribution (top),</br>
                           and the scalarized reward distribution (bottom)
                           of the hypernetwork based approach as a function
                           of timesteps during the leftmost experiment of 
                           figure a).
                        </figcaption>
                     </figure>
                     <ul class="main-fig-text-fig-layout-text">
                        <li>
                           Clearly, the SAP-critic-MO-hyper overfits on the 
                           energy efficiency objective early on. Even in the 
                           successful experiment, the first 1 million
                           steps are spend considering only policies that
                           optimize the energy efficiency.
                        </li> 
                        <li>
                           This indicates that the rewards from the energy
                           efficiency objective must be significantly larger than 
                           from the speed objective, which is 
                           verified by the scalarized reward distribution shown
                           in figure b).
                        </li>
                        <li>
                           Curiously, the CAPQL also is affected by the unbalanced
                           objectives, since the solution set contains almost 
                           optimal policies for saving energy early on. However,
                           it does not overfit on the easier objective.
                        </li>
                     </ul>
                  </div>
               </section>

               <section id="compacting-the-overfitting">
                 <h2>Overcoming the overfitting</h2> 
                 <div class="split-col-one-col-layout">
                    <div class="split-col-one-col-layout-text text-vbox">
                       <ul>
                          <li>
                           The overcome the overfitting, a warm-up period 
                           was added. During this period, a <em>skewed</em> 
                           preference distribution was used, where preferences
                           favoring the forward speed objective had a higher
                           change of getting selected.
                          </li>
                          <li>
                           After the warm-up period, the Gaussian distribution
                           from the original CAPQL was used for sampling 
                           the preferences.
                          </li>
                          <li>
                           The warm-up phase increases the initial convergence
                           speed significantly in Halfcheetah, while decreasing
                           the run-to-run variance simultaneously. However,
                           there is a significant drop in the performance soon
                           after the warm-up phase ends.
                          </li>
                          <li>
                           Moreover, the warm-up distribution improves the 
                           coverage of the solution set, yet it introduces 
                           a gap to the region of policies that favor the
                           energy efficiency.
                          </li>
                          <li>
                           Unfortunately, not even the warm-up phase could not
                           improve the performance in the Swimmer.
                          </li>
                       </ul>
                    </div>

                    <figure class="split-col-one-col-layout-fig2">
                     <img src="./images/warmup_plots/warmup-distribution-theory.svg" alt="">
                     <figcaption>
                        <b>Figure a)</b> Conceptual figure of the skewed
                        preference distribution. Preferences belonging on the 
                        red area were given 1.75 times higher change of being
                        selected than the preferences outside this area.
                     </figcaption>
                    </figure>

                     <figure class="split-col-one-col-layout-fig1">
                        <img src="./images/warmup_plots/sample_efficiency_talk_vertical.svg" alt="">
                        <figcaption>
                           <b>Figure b)</b> The performance of the hypernetwork
                           approach with and without the warm-up phase. The vertical
                           lines mark the end of warm-up phase. The bottom 
                           figures show an example of the produced solution
                           sets.
                        </figcaption>
                     </figure>
                 </div>
               </section>

               <section id="summary-and-future-directions">
                  <h2 style="margin-bottom: 0;">Summary & Future research directions</h2>
                  <span style="font-size: 50%">
                     <a href="https://github.com/SanteriHei/hypernet_morl">Code</a>
                  </span>
                  <div class="column-layout">
                     <figure>
                        <figcaption class="col-title">Summary</figcaption>
                        <ul>
                           <li>
                              Hypernetworks seem to be a promising option for 
                              improving the <em>convergence speed</em> of MORL
                              algorithms in more complicated tasks.
                           </li>
                           <li>
                              However, Hypernetworks <span class="alert">do not</span>
                              seem to improve the final performance over the base
                              algorithm significantly in all cases.
                           </li>
                           <li>
                              Hypernetworks are known to be fragile, and this
                              is the case also here.
                           </li>
                           <li>
                              The method used to select the preferences has a 
                              significant impact on the performance.
                           </li>
                        </ul>
                     </figure>
                     <figure>
                        <figcaption class="col-title">Future directions</figcaption>   
                        <ul>
                           <li>
                              This work considered using hypernetwork only for 
                              the critic. However, also the policy is 
                              conditioned on the preferences. 
                              Utilizing a hypernetwork for both policy and critic
                              could be an option worth looking into.
                           </li>  

                           <li>
                              Currently, GPI-LS offers unmatched sample-efficiency.
                              Applying hypernetworks to GPI-LS could provide
                              insight on how much the hypernetworks can
                              improve the convergence speed in the best case.
                           </li>
                           <li>
                              Continue to develop more general algorithm
                              for selecting the preferences during the training.
                           </li>
                        </ul> 
                     </figure>
                  </div>
               </section>
               
               <section id="references">
                  <h2> References </h2>
                  <p>
                     [1] D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley,
                     <span>“A survey of multi-objective sequential decision-making,”</span>
                     <em>Journal of Artificial Intelligence Research</em>, vol. 48, 2013.
                  </p>
                  <p>
                     [2] C. F. Hayes <em>et al.</em>, <span>“A practical guide
                     to multi-objective reinforcement learning and planning,”</span>
                     <em>Autonomous Agents and Multi-Agent Systems</em>, vol. 36, no. 1, 2022.
                  </p>
                  <p>
                     [3] X. Chen, A. Ghadirzadeh, M. Björkman, and P. Jensfelt,
                     <span>“Meta-learning for multi-objective reinforcement learning,”</span>
                     in <em>2019 IEEE/RSJ international conference on intelligent robots and systems (IROS)</em>,2019.
                  </p>
                  <p>
                     [4] V. K. Chauhan, J. Zhou, P. Lu, S. Molaei, and D. A. Clifton,
                     <span>“A brief review of hypernetworks in deep learning,”</span>
                     <em>Artificial Intelligence Review</em>, vol. 57, no. 6, 2024.
                  </p>

                   <p>
                     [5] H. Lu, D. Herman, and Y. Yu, <span>“Multi-objective reinforcement learning:
                     Convexity, stationarity and pareto optimality,”</span> in <em>The
                     eleventh international conference on learning representations</em>,
                     2023.
                  </p>

                  <p>
                     [6] E. Sarafian, S. Keynan, and S. Kraus, <span>“Recomposing the reinforcement
                     learning building blocks with hypernetworks,”</span> in <em>Proceedings
                     of the 38th international conference on machine learning</em>,
                     2021.
                  </p>
                  <p>
                     [7] J. Xu, Y. Tian, P. Ma, D. Rus, S. Sueda, and W. Matusik,
                     <span>“Prediction-guided multi-objective reinforcement learning for
                     continuous robot control,”</span> in <em>Proceedings of the 37th
                     international conference on machine learning</em>, 2020.
                  </p>

                  <p>
                     [8] A. B. Alegre Lucas N, D. M. Roijers, A. Nowé, and B. C. da Silva,
                     <span>“Sample-efficient multi-objective learning via generalized policy
                     improvement prioritization,”</span> in <em>AAMAS ’23: Proceedings of the
                     2023 international conference on autonomous agents and multiagent
                     systems</em>, 2023.
                  </p>

                  <p>
                     [9] L. M. Zintgraf, T. V. Kanters, D. M. Roijers, F. Oliehoek, and P. Beau,
                     <span>“Quality assessment of MORL algorithms: A utility-based
                     approach,”</span> in <em>Benelearn 2015: Proceedings of the 24th annual
                     machine learning conference of belgium and the netherlands</em>,
                     2015.
                  </p>

                  <p>
                     [10] E. Zitzler, J. Knowles, and L. Thiele, <span>“Quality assessment of pareto
                     set approximations,”</span> in <em>Multiobjective optimization:
                     Interactive and evolutionary approaches</em>, Springer, 2008.
                  </p>

                  <p>
                     [11] H.Ishibuchi, H. Masuda, Y. Tanigaki, and Y. Nojima, <span>“Modified
                     distance calculation in generational distance and inverted generational
                     distance,”</span> in <em>Evolutionary multi-criterion optimization</em>, 2015.
                  </p>
               </section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
        <script src="plugin/math/math.js"></script>
        <script src="plugin/references_my/references.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
                slideNumber: "c/t",
                totalTime: 60*15,
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX]
			});
		</script>
	</body>
</html>
